{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Clustering Strategy & Insight Generation\n",
    "**Topic:** Unsupervised Learning Optimization  \n",
    "**Course:** KPITB UETM - Week 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Distance Logic (Brainstorming)\n",
    "\n",
    "### 1. Can two data points be far apart in one feature but still belong to the same cluster? Explain.\n",
    "**Answer:** Yes, two points can be far apart in one dimension but very close in others. Most clustering algorithms (like K-Means) use a multi-dimensional distance metric (e.g., Euclidean distance). If the \"closeness\" in other features outweighs the distance in a single feature, the points will likely be grouped together. Additionally, in density-based clustering like DBSCAN, points can be far apart but connected via a chain of dense neighboring points.\n",
    "\n",
    "### 2. Why can unscaled features distort distance-based clustering results?\n",
    "**Answer:** Distance-based algorithms calculate the geometric distance between points. If one feature has a large range (e.g., Salary: $20,000 - $200,000) and another has a small range (e.g., Age: 18 - 80), the \"Salary\" feature will dominate the distance calculation. The algorithm will treat a difference of 100 units in salary as more significant than a difference of 50 units in age, even if the age difference is more meaningful for segmentation.\n",
    "\n",
    "### 3. List two reasons why feature scaling is critical before applying K-Means or Hierarchical Clustering.\n",
    "**Answer:**\n",
    "1. **Uniform Weighting:** Scaling ensures that no single feature dominates the distance metric simply because of its magnitude, allowing all features to contribute equally to the cluster formation.\n",
    "2. **Algorithm Convergence:** For K-Means, which is an iterative optimization algorithm using centroids, scaling helps the algorithm converge faster and find more stable, robust clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Development Challenge\n",
    "\n",
    "### Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette(\"viridis\") # Rich Aesthetics\n",
    "\n",
    "# Load Dataset\n",
    "url = \"https://raw.githubusercontent.com/tirthajyoti/Machine-Learning-with-Python/master/Datasets/Mall_Customers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.rename(columns={'Annual Income (k$)': 'Income', 'Spending Score (1-100)': 'Spend_Score'})\n",
    "\n",
    "# Select Features and Scale\n",
    "X = df[['Income', 'Spend_Score']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Data Preparation Complete.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The \u201cNo-Optimization\u201d Test\n",
    "Applying K-Means with an arbitrary value of **K=2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_no_opt = KMeans(n_clusters=2, random_state=42)\n",
    "labels_no_opt = kmeans_no_opt.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Income'], df['Spend_Score'], c=labels_no_opt, cmap='viridis', edgecolors='k')\n",
    "plt.title(\"K-Means Clustering (No Optimization, K=2)\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Spending Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The \u201cOptimized\u201d Test\n",
    "Using the **Elbow Method** and **Silhouette Score** to find the best K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow Plot\n",
    "ax[0].plot(k_range, wcss, marker='o', linestyle='--')\n",
    "ax[0].set_title('Elbow Method (WCSS)')\n",
    "ax[0].set_xlabel('Number of Clusters (K)')\n",
    "ax[1].set_ylabel('WCSS')\n",
    "\n",
    "# Silhouette Plot\n",
    "ax[1].plot(k_range, silhouette_scores, marker='o', color='purple')\n",
    "ax[1].set_title('Silhouette Score')\n",
    "ax[1].set_xlabel('Number of Clusters (K)')\n",
    "ax[1].set_ylabel('Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Elbow plot (the bend at K=5) and the highest Silhouette Score, the **optimal K is 5**.\n",
    "\n",
    "### Optimized Clustering Comparison (K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_opt = KMeans(n_clusters=5, random_state=42)\n",
    "labels_opt = kmeans_opt.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Income'], df['Spend_Score'], c=labels_opt, cmap='plasma', edgecolors='k')\n",
    "plt.title(\"Optimized K-Means Clustering (K=5)\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Spending Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis: Compare both approaches\n",
    "\n",
    "**1. Cluster Quality:**\n",
    "- **No-Optimization (K=2):** The clusters are too broad. They group people only into roughly \"high vs low score\" or \"low vs high income\" depending on the centroid initialization, failing to capture the distinct sub-segments in the middle and corners of the data.\n",
    "- **Optimized (K=5):** The clusters clearly separate the data into 5 distinct groups: High Income/High Spend, High Income/Low Spend, Low Income/High Spend, Low Income/Low Spend, and an Average group. The Silhouette score is mathematically much higher, indicating better defined clusters.\n",
    "\n",
    "**2. Interpretability:**\n",
    "- The **Optimized approach** produced far more meaningful clusters. management can now target \"Whales\" (High Income/High Spend) or \"Frugal Customers\" (High Income/Low Spend) with different strategies. A K=2 split is too vague for targeted decision-making.\n",
    "\n",
    "**3. Conclusion:** The **Optimized approach** is superior as it respects the natural geometry of the data and provides actionable business segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Algorithm Intuition\n",
    "\n",
    "**Question:** If K-Means produces different cluster assignments on different runs but Hierarchical Clustering produces the same structure every time, which result would you trust more for business reporting and why?\n",
    "\n",
    "**Answer:** For **business reporting consistency**, I would trust **Hierarchical Clustering** (specifically agglomerative) more for its **determinism**. K-Means is stochastic; it depends on the random initialization of centroids, which refers to the \"Lucky/Unlucky Seeding\" problem. While K-Means is faster for large data, a change in reporting results every time the script runs can confuse management. However, for **segment quality**, K-Means with a fixed `random_state` is often preferred if it produces better-separated groups. In a strictly reporting context where consistency is king, the deterministic nature of Hierarchical Clustering makes it highly reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Report Summary\n",
    "\n",
    "1. **Clustering Approach used:** K-Means Clustering with feature scaling (StandardScaler).\n",
    "2. **Method for selecting K:** Combined the **Elbow Method** (within-cluster sum of squares) and the **Silhouette Score** (measuring cluster density and separation).\n",
    "3. **Meaningfulness:** The final clusters (K=5) represent 5 logical consumer behaviors (Frugal, Rich Spenders, Careful Spenders, Impulsive, and Target Balanced). This is significantly more useful for marketing than arbitrary segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
